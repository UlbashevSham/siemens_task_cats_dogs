{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c98cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d764958",
   "metadata": {},
   "source": [
    "## В этом ноутбуке мы обучим и сохраним модель классификации породы собак на изображении. Будем файн-тьюнить Resnet50. Обучим модель на датасете с породами собак https://www.kaggle.com/competitions/dog-breed-identification/overview, а применить попробуем к нашему дата сету с кошками и собаками. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd7a6d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (0.20.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (0.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (9.0.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (2.26.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (3.0)\n",
      "Requirement already satisfied: scipy<1.9.2,>=1.8 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.9.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (2023.3.15)\n",
      "Requirement already satisfied: numpy>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.22.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->scikit-image) (3.0.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93135f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from skimage import io\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "281df6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping labels same order of sample_submittion.csv\n",
    "map_labels = dict()\n",
    "for index, value in enumerate(pd.read_csv('../data/dog-breed-identification/sample_submission.csv').columns[1:]):\n",
    "    map_labels[value] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e7a24e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load custom dataset\n",
    "class DogBreedsDataset_train(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.labels = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform['train']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root_dir, self.labels.iloc[index, 0])+'.jpg'\n",
    "        image = io.imread(img_path)\n",
    "        y_label = torch.tensor(int(map_labels[self.labels.iloc[index, 1]]))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c02a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load custom dataset\n",
    "class DogBreedsDataset_test(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.name_images = pd.read_csv(csv_file)['id'].values\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform['test']\n",
    "    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.name_images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root_dir, self.name_images[index])+'.jpg'\n",
    "        image = io.imread(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23b40de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "in_channel = 3\n",
    "num_classes = 120\n",
    "learning_rate = 5*1e-4\n",
    "batch_size = 128\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9c41772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images preprocessing\n",
    "transform ={'train': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.95, 1.0)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),  # Image net standards\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])]),  # Imagenet standards\n",
    "    'test': transforms.Compose([transforms.ToPILImage(),\n",
    "                                      transforms.RandomResizedCrop(size=256, scale=(0.95, 1.0)),\n",
    "                                      transforms.CenterCrop(size=224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                          [0.229, 0.224, 0.225])])}\n",
    "\n",
    "dataset = DogBreedsDataset_train('../data/dog-breed-identification/labels.csv', '../data/dog-breed-identification/train', transform)\n",
    "# Random split data to 70% training and 30% validation\n",
    "train_set_size = int(len(dataset) * 0.7)\n",
    "valid_set_size = len(dataset) - train_set_size\n",
    "train_set, valid_set = torch.utils.data.random_split(dataset, [train_set_size, valid_set_size])\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# loading testing data\n",
    "test_data = DogBreedsDataset_test('../data/dog-breed-identification/sample_submission.csv', '../data/dog-breed-identification/test', transform=transform)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e714fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device cuda for GPU if it's available otherwise run on the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2c374d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=120, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model architecture with weights (resnet50) \n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# freeze all layers \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# replace last layer with from 1000 classes to be 120 classes\n",
    "model.fc = nn.Linear(2048, 120)\n",
    "model = model.to(device=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a99199a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# checkpoint to save and load model\n",
    "checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adb87f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traindata(device, model, epochs, optimizer, loss_function, train_loader, valid_loader):\n",
    "    # Early stopping\n",
    "    best_loss = 100\n",
    "    patience = 5\n",
    "    triggertimes = 0\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        loss_total = 0\n",
    "        for data in train_loader:\n",
    "            input = data[0].to(device)\n",
    "            label = data[1].to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward and backward propagation\n",
    "            output = model(input)\n",
    "            _, predicted = output.max(1)\n",
    "            loss = loss_function(output, label)\n",
    "            loss_total += loss.item()\n",
    "            correct+= (predicted == label).sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('epoch number: {}'.format(epoch))\n",
    "        print('Training Accuracy: {} Training loss: {}'.format(correct/len(train_loader.sampler), loss_total/len(train_loader)))\n",
    "        \n",
    "        # Early stopping\n",
    "        current_loss = validation(model, device, valid_loader, loss_function)\n",
    "        print('The Current Loss:', current_loss)\n",
    "        print('Best Loss:', best_loss)\n",
    "\n",
    "        if current_loss > best_loss:\n",
    "            trigger_times += 1\n",
    "            print('Trigger Times:', trigger_times)\n",
    "\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!\\nStart to test process.')\n",
    "                return model\n",
    "\n",
    "        else:\n",
    "            print('trigger times: 0')\n",
    "            trigger_times = 0\n",
    "            #save_checkpoint(checkpoint)\n",
    "            best_loss = current_loss\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac58841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, device, valid_loader, loss_function):\n",
    "\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    correct = 0\n",
    "    # Test validation data\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            input = data[0].to(device)\n",
    "            label = data[1].to(device)\n",
    "\n",
    "            output = model(input)\n",
    "            _, predicted = output.max(1)\n",
    "            loss = loss_function(output, label)\n",
    "            loss_total += loss.item()\n",
    "            correct+= (predicted == label).sum()\n",
    "        print('Validation Accuracy: {}'.format(correct/len(valid_loader.sampler)))\n",
    "    return loss_total / len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95ff5bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 1\n",
      "Training Accuracy: 0.75052410364151 Training loss: 1.6510114989110403\n",
      "Validation Accuracy: 0.7574176788330078\n",
      "The Current Loss: 1.3101984361807506\n",
      "Best Loss: 100\n",
      "trigger times: 0\n",
      "epoch number: 2\n",
      "Training Accuracy: 0.8132774233818054 Training loss: 1.0687778634684426\n",
      "Validation Accuracy: 0.7896968126296997\n",
      "The Current Loss: 0.9936371048291525\n",
      "Best Loss: 1.3101984361807506\n",
      "trigger times: 0\n",
      "epoch number: 3\n",
      "Training Accuracy: 0.8401117920875549 Training loss: 0.8409305544836181\n",
      "Validation Accuracy: 0.7932833433151245\n",
      "The Current Loss: 0.8937988728284836\n",
      "Best Loss: 0.9936371048291525\n",
      "trigger times: 0\n",
      "epoch number: 4\n",
      "Training Accuracy: 0.8554856777191162 Training loss: 0.7112004033156804\n",
      "Validation Accuracy: 0.8095859289169312\n",
      "The Current Loss: 0.7838404253125191\n",
      "Best Loss: 0.8937988728284836\n",
      "trigger times: 0\n",
      "epoch number: 5\n",
      "Training Accuracy: 0.8679245114326477 Training loss: 0.621671157756022\n",
      "Validation Accuracy: 0.8004564642906189\n",
      "The Current Loss: 0.7607966214418411\n",
      "Best Loss: 0.7838404253125191\n",
      "trigger times: 0\n",
      "epoch number: 6\n",
      "Training Accuracy: 0.8705800175666809 Training loss: 0.5766086088759559\n",
      "Validation Accuracy: 0.7975220084190369\n",
      "The Current Loss: 0.7331444397568703\n",
      "Best Loss: 0.7607966214418411\n",
      "trigger times: 0\n",
      "epoch number: 7\n",
      "Training Accuracy: 0.883298397064209 Training loss: 0.518106993819986\n",
      "Validation Accuracy: 0.8076296448707581\n",
      "The Current Loss: 0.6912222852309545\n",
      "Best Loss: 0.7331444397568703\n",
      "trigger times: 0\n",
      "epoch number: 8\n",
      "Training Accuracy: 0.8879105448722839 Training loss: 0.48285241584692684\n",
      "Validation Accuracy: 0.8007825613021851\n",
      "The Current Loss: 0.6860813026626905\n",
      "Best Loss: 0.6912222852309545\n",
      "trigger times: 0\n",
      "epoch number: 9\n",
      "Training Accuracy: 0.8936407566070557 Training loss: 0.455783520426069\n",
      "Validation Accuracy: 0.8105641007423401\n",
      "The Current Loss: 0.6620159472028414\n",
      "Best Loss: 0.6860813026626905\n",
      "trigger times: 0\n",
      "epoch number: 10\n",
      "Training Accuracy: 0.8995108008384705 Training loss: 0.4312809780240059\n",
      "Validation Accuracy: 0.8102380037307739\n",
      "The Current Loss: 0.6596581141153971\n",
      "Best Loss: 0.6620159472028414\n",
      "trigger times: 0\n",
      "epoch number: 11\n",
      "Training Accuracy: 0.9024457931518555 Training loss: 0.40310320471014294\n",
      "Validation Accuracy: 0.8138245940208435\n",
      "The Current Loss: 0.6443315992752711\n",
      "Best Loss: 0.6596581141153971\n",
      "trigger times: 0\n",
      "epoch number: 12\n",
      "Training Accuracy: 0.9083158373832703 Training loss: 0.3850667519228799\n",
      "Validation Accuracy: 0.8115422129631042\n",
      "The Current Loss: 0.6451142728328705\n",
      "Best Loss: 0.6443315992752711\n",
      "Trigger Times: 1\n",
      "epoch number: 13\n",
      "Training Accuracy: 0.9112508296966553 Training loss: 0.3678932354918548\n",
      "Validation Accuracy: 0.8128464221954346\n",
      "The Current Loss: 0.6412625710169474\n",
      "Best Loss: 0.6443315992752711\n",
      "trigger times: 0\n",
      "epoch number: 14\n",
      "Training Accuracy: 0.9180992245674133 Training loss: 0.3490764272532293\n",
      "Validation Accuracy: 0.8082817196846008\n",
      "The Current Loss: 0.6387048959732056\n",
      "Best Loss: 0.6412625710169474\n",
      "trigger times: 0\n",
      "epoch number: 15\n",
      "Training Accuracy: 0.9250873327255249 Training loss: 0.3272441537784679\n",
      "Validation Accuracy: 0.8141506314277649\n",
      "The Current Loss: 0.6310397659738859\n",
      "Best Loss: 0.6387048959732056\n",
      "trigger times: 0\n",
      "epoch number: 16\n",
      "Training Accuracy: 0.9273235201835632 Training loss: 0.3193242797361953\n",
      "Validation Accuracy: 0.8131725192070007\n",
      "The Current Loss: 0.6318995965023836\n",
      "Best Loss: 0.6310397659738859\n",
      "Trigger Times: 1\n",
      "epoch number: 17\n",
      "Training Accuracy: 0.9277428388595581 Training loss: 0.30692269067679134\n",
      "Validation Accuracy: 0.8174111843109131\n",
      "The Current Loss: 0.6186445777614912\n",
      "Best Loss: 0.6310397659738859\n",
      "trigger times: 0\n",
      "epoch number: 18\n",
      "Training Accuracy: 0.9288609027862549 Training loss: 0.29298466231141773\n",
      "Validation Accuracy: 0.8134985566139221\n",
      "The Current Loss: 0.6218411202232043\n",
      "Best Loss: 0.6186445777614912\n",
      "Trigger Times: 1\n",
      "epoch number: 19\n",
      "Training Accuracy: 0.9306778311729431 Training loss: 0.28912604014788357\n",
      "Validation Accuracy: 0.8056733012199402\n",
      "The Current Loss: 0.6484781118730704\n",
      "Best Loss: 0.6186445777614912\n",
      "Trigger Times: 2\n",
      "epoch number: 20\n",
      "Training Accuracy: 0.9338923692703247 Training loss: 0.27758784698588507\n",
      "Validation Accuracy: 0.8056733012199402\n",
      "The Current Loss: 0.6253083944320679\n",
      "Best Loss: 0.6186445777614912\n",
      "Trigger Times: 3\n",
      "epoch number: 21\n",
      "Training Accuracy: 0.9380852580070496 Training loss: 0.26868500533912865\n",
      "Validation Accuracy: 0.8095859289169312\n",
      "The Current Loss: 0.6227070440848669\n",
      "Best Loss: 0.6186445777614912\n",
      "Trigger Times: 4\n",
      "epoch number: 22\n",
      "Training Accuracy: 0.939063549041748 Training loss: 0.2543274954493557\n",
      "Validation Accuracy: 0.8099119663238525\n",
      "The Current Loss: 0.6083542456229528\n",
      "Best Loss: 0.6186445777614912\n",
      "trigger times: 0\n",
      "epoch number: 23\n",
      "Training Accuracy: 0.9431166648864746 Training loss: 0.24158235053930963\n",
      "Validation Accuracy: 0.8046951293945312\n",
      "The Current Loss: 0.6213718007008234\n",
      "Best Loss: 0.6083542456229528\n",
      "Trigger Times: 1\n",
      "epoch number: 24\n",
      "Training Accuracy: 0.944514274597168 Training loss: 0.2327269861208541\n",
      "Validation Accuracy: 0.8076296448707581\n",
      "The Current Loss: 0.626993623872598\n",
      "Best Loss: 0.6083542456229528\n",
      "Trigger Times: 2\n",
      "epoch number: 25\n",
      "Training Accuracy: 0.9442347884178162 Training loss: 0.23279349213199957\n",
      "Validation Accuracy: 0.8063254356384277\n",
      "The Current Loss: 0.6189440612991651\n",
      "Best Loss: 0.6083542456229528\n",
      "Trigger Times: 3\n",
      "epoch number: 26\n",
      "Training Accuracy: 0.9512228965759277 Training loss: 0.2197688426822424\n",
      "Validation Accuracy: 0.8141506314277649\n",
      "The Current Loss: 0.6208481366435686\n",
      "Best Loss: 0.6083542456229528\n",
      "Trigger Times: 4\n",
      "epoch number: 27\n",
      "Training Accuracy: 0.9492661952972412 Training loss: 0.21449624534164155\n",
      "Validation Accuracy: 0.8056733012199402\n",
      "The Current Loss: 0.6206645493706068\n",
      "Best Loss: 0.6083542456229528\n",
      "Trigger Times: 5\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "CPU times: user 1h 7min 12s, sys: 6min 36s, total: 1h 13min 49s\n",
      "Wall time: 1h 20min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = traindata(device, model, num_epochs, optimizer, criterion, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d5cb8",
   "metadata": {},
   "source": [
    "## Сохраним модель и вызовем в 3_evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "199e60f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../data/dogs_breed\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
